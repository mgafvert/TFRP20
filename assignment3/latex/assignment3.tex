\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% \usepackage[swedish]{babel}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{url}
\usepackage{amsmath}

\addbibresource[]{assignment3.bib}
\fvset{tabsize=4}
\fvset{fontsize=\small}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\title{TFRP20 Artificial Intelligence (VT25)\\ Assignment 3: Classification with the perceptron and logistic regression}
\author{Anders Holm\\\texttt{anders.holm@senspix.net} \and
Magnus Gäfvert\\\texttt{magnus.gafvert@gmail.com}}
\date{2025--03--10}
\begin{document}
\maketitle
\begin{abstract}
    This report present and discuss the authors' solution to an assignment on classification with the perceptron and logistic regression in teh Artificial Intelligence course given at the Department of Computer Science at Lund University in the spring of 2025. The solution is implemented in the given template Jupyter notebook and provided alongside this report.
\end{abstract}

\section{Introduction}
The objectives of this assignment~\cite{tfrp20assignment3} are to:

\begin{enumerate}
    \item  Write a linear regression program using gradient descent;
    \item  Write linear classifiers using the perceptron algorithm and logistic regression;
    \item  Experiment variations of the algorithms;
    \item  Evaluate classifiers;
    \item  Experiment with popular tools;
    \item  Read a scientific article on optimization techniques and comment it;
    \item  Present code, results, and comments in a short dissertation. 
\end{enumerate}

The respective tasks are presented in the following sections.

\section{Method}
The implementation toolchain is based on Jupyter Notebook with numpy with Python 3.9.21 and the Anaconda distribution~\cite{anaconda} using Microsoft Visual Studio Code with free GitHub Copilot served by the preview Anthropic Claude 3.5 Sonnet LLM and Microsoft Python plugins. This report is typeset with \LaTeX using TexLive and and the LaTeX Workshop plugin by James Yu.

The code is structured in a Jupyter notebook: 
\begin{verbatim}
    ai_lab_perceptron_students.ipynb    
\end{verbatim}
with sections and cells for each of the tasks in the assignment, and handed in together with this report. The code is evaluated with the provided evaluation data from the historial novel Salammbô by Gustave Flaubert~\cite{flaubert1862salammbô} in original French and in English translation to verify performance of the implementation.

 
\section{Results and discussion}


\subsection{Linear regression program using gradient descent}

Linear regression with gradient descent in batch and stochastic variants as described in \cite[pp. 694--697]{aima} are implemented based on~\cite{nugues_lectures_2025} in the Jupyter notebook. The algorithms are applied on the provided word and letter 'a' counts from the 15 chapters of~\cite{flaubert1862salammbô} in French and English. The result is shown in Figure~\ref{fig:regression_English_French} with final weights  summarized in Table~\ref{tab:regression-table}, and with convergence speeds illustrated in Figure~\ref{fig:regression_errors}.  For this small multi-variable data set the result could also be verified by comparing with an exact solution from the normal equation\cite[Equation (19.7)]{aima}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Fit English.png}
    \includegraphics[width=0.45\textwidth]{figures/Fit French.png}
    \caption{Linear regression of letter 'a' and word counts with batch and gradient descent trained on English translation (left) and French original (right) of cite{flaubert1862salammbô}.}
    \label{fig:regression_English_French}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Regression errors.png}
    \caption{Comparison of onvergence of the batch and stochastic versions of linear regression trained on \cite{flaubert1862salammbô}.}
    \label{fig:regression_errors}
\end{figure}

\begin{table}[]
    \caption{Regression results}
    \label{tab:regression-table}
    \small
    \begin{tabular}{lllll}
        \hline\hline
             & \multicolumn{2}{c}{French}                    & \multicolumn{2}{c}{English}            \\
             & Gradient descent & Stochastic descent & Gradient descent & Stochastic descent \\
             \hline
    Epoch    & 187              & 31                 & 186              & 15                 \\
    w{[}0{]} & 0.00170945       & -0.00170833        & -6.71530774e-04  & 0.00111656         \\
    w{[}1{]} & 0.98640986       & 0.99526806         & 9.94590530e-01   & 1.00362464\\
    \hline
    \end{tabular}
\end{table}


\subsection{Linear classifiers using the perceptron algorithm and logistic regression}
The stochastic perceptron program with hard threshold as described in \cite[pp. 700--702]{aima} is implemented based on~\cite{nugues_lectures_2025} in the Jupyter notebook as suggested with \verb|fit(X,y)| and \verb|predict(X,w)| functions. The algorithms are applied on the provided word and letter 'a' counts from the 15 chapters of~\cite{flaubert1862salammbô} in French and English. The result is shown in Figure~\ref{fig:Stochastic_descent} (top) with successful separation of the data points for French and English. Leave-one-out cross-validation is shown in Figure~\ref{fig:classifier_result} (top), with 100\% success rate.

The stochastic logistic regression program as described in \cite[pp. 702--705]{aima} is likewise implemented based on~\cite{nugues_lectures_2025} in the Jupyter notebook. The algorithms are applied on the same data set. The result is shown in Figure~\ref{fig:Stochastic_descent_logistic_classifier} with successful separation of the data points for French and English. Leave-one-out cross-validation is shown in Figure~\ref{fig:classifier_result}, with 100\% success rate. The logistic surface is illustrated in Figure~\ref{fig:logistic_classifier}.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Stochastic descent hard threshold classifier.png}
    \includegraphics[width=0.45\textwidth]{figures/Stoschastic descent logistic classifier.png}
    \caption{Linear classifier with hard threshold (left) and logistic function (right).}
    \label{fig:Stochastic_descent}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/classifier result 1.png}
    \includegraphics[width=0.8\textwidth]{figures/classifier result 2.png}
    \caption{Leave-one-out cross-validation of the perceptron algorithm with hard threshold (top) and logistic function (bottom).}
    \label{fig:classifier_result}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/logistic classifier.png}
    \caption{Screenshot of the Jupyter notebook with the implementation of the perceptron algorithm.}
    \label{fig:logistic_classifier}
\end{figure}

We could not, or at least not without confusing tweaking of parameters, get a gradient descent algorithm to converge to a solution. The stochastic descent algorithm described above, however, worked. The convergence criteria we used was the average of the gradient every epoch.

\section{Experimenting with popular tools}
Three ready-to-run notebooks are provided in the assignment to demonstrate how three popular tools for machine learning can be used for the Salammbô exercise: Scikit-learn, Keras, and PyTorch. The authors have studied and executed the notebooks, but not spent time on exploring these tools in any depth. It is clear that they provide extensive and powerful high-level abstractions for machine learning algorithms.

\subsection{\texttt{Scikit\_learn}}
\texttt{Scikit\_learn} includes fitting and prediction functions similiar to those implemented in the assignment. 

\subsection{\texttt{Keras}}
\texttt{Keras} is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It allows for easy and fast prototyping. The authors have not used \texttt{Keras} before, but it appears to be a powerful tool for deep learning.

\subsection{\texttt{PyTorch}}
\texttt{PyTorch} is an open source machine learning library based on the Torch library. It is used for applications such as natural language processing. The authors have not used \texttt{PyTorch} before, but it appears to be a powerful tool for deep learning.



\section{Reading Ruder}

In this section, we summarize and discuss the article by Ruder~\cite{DBLP:journals/corr/Ruder16} on optimization techniques for gradient descent.
A main take-away is that most descent optimization algorithms or tweaks seem to be experimentally verified more than analytically derived, i.e an author has tried a tweak that on some general level seems understandable even for a beginner.

% The mathematical notation in the article is somewhat of barrier for the beginner in the field. 

The vanilla \emph{batch gradient descent} is appropriate for smaller (fit-in-memory) datasets. The main advantage is that the approach is guaranteed to converge to a global minimum for convex problems (and local minimum for non-convex problems).\footnote{However, we did not see this for the perceptron fitting on the Salammbô data.} The \emph{stochastic gradient descent} (SGD) is more appropriate for larger datasets and online use as it works with one sample at a time, but may require more tuning of the learning rate. The \emph{mini-batch gradient descent} is a compromise between the two and works with a small set of samples at a time. However, a major challenge is the choice of learning rate to get acceptable convergence.
  

The article summarizes the challenges in Gradient Descent Optimization (GDO), irrespective of using \emph{vanilla GD}, \emph{SGD} or a mini-batch variant of GD:
\begin{itemize}
    \item How to choose an appropriate learning rate and and possibly modifying it over time
    \item  How to escape saddle points and local minima
    \item  How to handle sparse data
\end{itemize}

\subsection{Gradient descent algorithms}
Some specific algorithms are discussed in the article, and we will briefly summarize them below. Ruder visualizes the performance of the algorithms by plotting their trajectories in the parameter space for a benchmark loss function, and concludes that adaptive learning rate methods provide best convergence. Overall, Ruder recommends Adam as a robust and efficient optimization algorithm, but remarks that many recent papers use vanilla SGD with simple learning rate adjustments.

\subsubsection{Momentum}
Accelerates convergence by adding a fraction of the previous update to the current one.
Ruder claims that this helps overcome local minima and accelerates progress along consistent directions. This optimization approach also appears to be easy to implement.
A couple of lines of code in the AI-lab notebook would have sufficed.

\subsubsection{Nesterov Accelerated Gradient (NAG)}
A smarter version of momentum with a \emph{look ahead}, i.e partly \emph{skating to where the puck/parameters will be}~\cite{hinton_slides_2012}. 
This approach would also have been fairly simple to implement in the lab.

\subsubsection{Adagrad and Adadelta}
Adagrad adapts learning rates \emph{per-parameter} based on historical gradients. It eliminates the need to manually tune the learning rate, but accumulation in the denominator results in a ever decreasing learning rate which eventually stops further learning. Adadelta is an extension of Adagrad that seeks to reduce this problem by using a moving average instead of accumulation. Adadelta does not even need a default learning rate.


\subsubsection{RMSprop}
RMSprop is an unpublished extension of Adagrad also aiming to reduce the learning rate decay by an exponential function. 

\subsubsection{Adam}
Adam is a combination of RMSprop and momentum. It computes adaptive learning rates for each parameter and stores an exponentially decaying average of past squared gradients. It is computationally efficient and has been shown to work well in practice.

\subsubsection{AdaMax}
AdaMax is a variant of Adam based on the infinity norm.

\subsubsection{Nadam}
Nadam is a variant of Adam based on Nesterov momentum. The paper elaborates on details of the update rule but does not clearly comment on the performance. 

\subsection{Parallelization, distributed SGD and additional strategies}
Ruder also discusses parallelization and distributed SGD, and lists and comments briefly on a number of implementations.

Among common tricks to improve performance of SGD, Ruder also mentions shuffling of input data at each epoch to avoid bias, although in some cases (progressively harder problems) a carefully selected order may improve learning.
Batch normalization is suggested to avoid slower training as a result of losing initial normalization over time.
Early stopping by monitoring the validation error is suggested to save computation (and avoid overfitting).
Adding noise to the gradients is suggested to avoid getting stuck in local minima.

\section{Conclusions and future work}
It is clear from this assignment that there is a vast body of theory and implementations in the machine learning domain, and that this course merely scrapes the surface. The course has equipped us with initial and basic understanding to explore further and dig deeper into the field, and to use the available tools and implementations.  

\printbibliography
\end{document}